{
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<img style=\"float: center; width: 100%\" src=\"https://raw.githubusercontent.com/andrejkk/TalksImgs/master/FrontSlideUpperBan.png\">\n<p style=\"margin-bottom:2cm;\"></p>\n\n<center>\n    <H1> 8. Elements of probability theory </H1>\n   \n\n\n<br><br>\n    <H3> Andrej Ko≈°ir, Lucami, FE </H4>\n    \n    <H4> Contact: prof. dr. Andrej Ko≈°ir, andrej.kosir@lucami.fe.uni-lj.si, skype=akosir_sid </H4>\n</center>\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 1 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> Goals </div>\n</div>\n\n\n## ‚ñ† Goals\n\n- Goals: \n    - Summarize basics of probability theory \n    - Summarize basics of random variable modeling\n\n\n\n- Probability space, probability, Random variable\n    - Discrete, continuous\n\n\n\n- Conditional probability\n- Bayes formula\n- Distributions, moments\n- Stochastic processes\n- Time series ‚Äì discrete time\n- Hypotheses testing\n\n\n\n- Prerequisite for \n    - Markov chains \n    - Time series ‚Äì TC traffic models\n    - Experiments with end users \n\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 2 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> Sections </div>\n</div>\n\n\n## ‚ñ† Sections\n\n8.1. Introduction\n\n‚ñ† Probability theory history\n\n‚ñ† Intuitive introduction ‚Äì voltage measurement example\n\n‚ñ† Different uses of probability\n\n\n8.2. Probability space, random variables\n\n‚ñ† Probability space and probability\n\n‚ñ† Random variable\n\n‚ñ† Distribution and distribution density\n\n‚ñ† Conditional probability and Bayes formula\n\n‚ñ† Momentums ‚Äì expectation and variance\n\n‚ñ† Sequence of random variabless \n\n‚ñ† Selected probability distributions\n\n\n8.3. Hypotheses testing \n\n‚ñ† Problem: is the difference significant?\n\n‚ñ† Null Hylpothesis, p-value\n\n‚ñ† Risk level $\\alpha$, \n\n‚ñ† Decission, errors\n\n‚ñ† sample size determination \n\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 3 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.1. Introduction </div>\n</div>\n\n\n## 8.1. Introduction\n\n‚ñ† Probability theory history\n\n‚ñ† Intuitive introduction ‚Äì voltage measurement example\n\n‚ñ† Different uses of probability\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 4 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.1. Introduction </div>\n</div>\n\n\n## ‚ñ† Probability theory history\n\n- 17. century: dice does not go as it should B. Pascal, P. Fermat, C. de Mere\n- A sum of three dices, the probability of getting 11 and getting 12\n    - The same number of combination for 11 and 12\n    - The relative frequency tend to be different!\n\n<table style=\"width:30%\">\n  <tr>\n    <th>$S=11$</th>\n    <th>$S=12$</th> \n  </tr>\n  <tr>\n    <td>$146$</td> \n    <td>$156$</td>\n  </tr>\n  <tr>\n    <td>$236$</td> \n    <td>$246$</td>\n  </tr>\n  <tr>\n    <td>$155$</td> \n    <td>$336$</td>\n  </tr>\n  <tr>\n    <td>$245$</td> \n    <td>$246$</td>\n  </tr>\n  <tr>\n    <td>$335$</td> \n    <td>$345$</td>\n  </tr>\n  <tr>\n    <td>$443$</td> \n    <td>$354$</td>\n  </tr>\n</table>\n\n\n\n- Solution: independence of events:\n    - Outcome $444$ is less frequent as $156$\n    - Definition: independent events if $ùëÉ[ùê¥ùêµ]=ùëÉ[ùê¥]ùëÉ[ùêµ]$\n    \n    \n- C. de Mere discovered statistical definition of probability\n\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 5 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.1. Introduction </div>\n</div>\n\n\n## ‚ñ† Intuitive intro - a case of voltage measurement\n\n\n- We measure a constant voltage\n- There is a noise in a measurement\n- Steps\n    1. A set of measurements: $\\{1.92, 2.03, ....\\}$;\n    2. Measurement model: \n    $$ u_i = u_0 + \\varepsilon_i $$\n    3. Histogram, relative frequencies\n    4. Random variable $U$ and its realization $u_i$ \n    5. Probability density: the description how random variable behaves\n    6. Event: conditional realizations of \n    $$ u \\in [1.93, 2.081] $$\n    7. Probability of the event:  \n    $$ P(U \\in [1.93, 2.081])=0.61; $$\n    8. Probability distribution\n    $$ F_U (u) = P[U \\leq u], $$\n    Probability density: \n    $$ p_U(ùë¢) = \\frac{F_U(u)}{d u}, $$\n    it fits to the normalized histogram;\n\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 6 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.1. Introduction </div>\n</div>\n\n\n\n## ‚ñ† Different definitions of probability\n\n- Statistical definition of probability: \n$$ P[A] = \\frac{n_k}{n} $$\n    - A law of large numbers\n\n\n\n<img style=\"float: center; width: 300px; margin: 0px 20px 20px 0px;\" src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/EstimateProbRatio.png\">\n\n\n- Geometrical definition of probability: \n $$ P[A] = \\frac{m(A)}{m(G)} $$\n    - Good for mathematical definition\n    - Monte Carlo Method\n\n\n\n<img style=\"float: center; width: 300px; margin: 0px 20px 20px 0px;\" src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/EstimateProbPi.png\">\n\n\n- Mathematical definition: \nevents are sets\n\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 7 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.2. Probability space, random variables </div>\n</div>\n\n\n## 8.2. Probability space, random variables\n\n‚ñ† Probability space and probability\n\n‚ñ† Random variable\n\n‚ñ† Distribution and distribution density\n\n‚ñ† Conditional probability and Bayes formula\n\n‚ñ† Conditional probability and Bayes formula\n\n‚ñ† Momentums ‚Äì expectation and variance\n\n‚ñ† Sequence of random variables \n\n‚ñ† Selected probability distributions\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 8 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.2. Probability space, random variables </div>\n</div>\n\n\n## ‚ñ† Probability space and probability\n\n\n- Probability space $G$\n\n\n- Event  $A\\subset G$, $A\\in \\cal G$\n\n\n- A set of events $\\cal G$\n    - Sure event:\n    $$ G\\in \\cal G $$\n    - Complement: \n    $$ A\\in {\\cal G} \\Rightarrow A^c\\in \\cal G $$\n    - Union: \n    $$ A_i\\in\\cal G \\Rightarrow \\cup_{i=1}^n A_i\\in \\cal G $$ \n\n\n\n\n<img style=\"float: right; width: 200px; margin: 0px 20px 20px 0px;\" src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/ConceptProbability.png\">\n\n\n\n- Probability: $P: \\cal G \\to [0,1]$\n    - Additive: \n    $$ P\\left(\\cup_{i=1}^n A_i\\right) = \\sum_{i=1}^n P(A_i) $$ \n    - We have \n    $$ P(G)=1, P(\\emptyset) = 0 $$\n    - Complement \n    $$ P(A^c) = 1 - P(A) $$\n   \n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 9 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.2. Probability space, random variables </div>\n</div>\n\n\n## ‚ñ† Random variable\n\n\n- Random variable: \n    $$ X:\\cal G \\to ‚Ñù $$\n    - Basic requirement ‚Äì ‚Äûmeasurable‚Äú:\n    $$ X^{-1}([a,b)) = [X\\in [a,b)] \\in\\cal G $$ \n\n\n- Continuous, discrete\n    - Continuous : voltage\n    - Discrete : user event\n    \n    \n- Realization of random variable\n\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 10 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.2. Probability space, random variables </div>\n</div>\n\n\n## ‚ñ† Distribution and distribution density\n\n\n- Random variable, histogram\n- Distribution: \n $$ F_f(x) = P[X\\leq x] $$\n - By definition \n $$ P[a \\leq X \\leq b] = F_X(b) - F_X(a) $$\n\n\n- Density function: \n    - continuous, \n    $$ P[a < X \\leq b] = \\int_a^b p_X(x) dx $$\n    - discrete:\n    $$ P[a < X \\leq b] = \\sum_{k\\in\\{a,\\ldots b\\}} p_k  $$\n    \n    \n    \n- Cont. distributions:\n    - Normal (Gauss)\n    - Chi squared\n\n- Discrete distributions:\n    - Bernoulli\n    - Poisson\n\n\n\n<img style=\"float: right; width: 300px; margin: 0px 20px 20px 0px;\" src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/HistAndProbDensity.png\">\n\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 11 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.2. Probability space, random variables </div>\n</div>\n\n\n## ‚ñ† Independent events, calculating events\n\n\n- Events $A,B\\in \\cal G$ are independent if\n$$ P[A B] = P[A] P[B] $$\n\n\n- Support intuition:\n    - Probability does not change if they occur together or not\n    \n\n- Calculating with events: \n    - $A$ or $B$ is $A\\cup B$\n    - $A$ and $B$ is $A\\cap B = A B$\n\n\n- Basic relation\n$$ P[A\\cup B] = P[A] + P[B] - P[A B] $$\n\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 12 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.2. Probability space, random variables </div>\n</div>\n\n\n## ‚ñ† Conditional probability and Bayes formula\n\n\n- Conditional probability \nIf je $P[B]>0$, then  \n$$ P[A|_B] = \\frac{P[AB]}{P[B]} $$ \n\n\n- We have \n$$ P[A_1 A_2 \\cdots A_n] = P[A_1] P[A_2|_{A_1}] P[A_3|_{A_1 A_2}] \\cdots \n  P[A_n|_{A_1 \\cdots A_{n-1}}] $$\n\n\n- Law of total probability\n    - Complete set of events ‚Äì hypotheses: $\\{H_1, \\ldots H_n\\}$ \n    - Formula: \n    $$ P[A] = \\sum_{i=1}^n P[A|_{H_i}] P[H_i] $$\n\n\n- Bayes formula\n$$ P[H_{k}|_{a}] = \\frac{P[A|_{H_k}] P[H_k]}{\\sum_{i=1}^n P[A|_{H_i}] P[H_i]} $$\n\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 13 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# -*- coding: utf-8 -*-\n\"\"\"\n@author: andrejk\n\"\"\"\n\n# Hypotheses and conditionals\n# Probabiliries of hypothese\nPr_H1 = 0.6\nPr_H2 = 0.3\nPr_H3 = 0.1\n\n# Conditional probabilities\nPr_AH1 = 0.03\nPr_AH2 = 0.01\nPr_AH3 = 0.02\n\n# Total probability of event A\nPr_A = Pr_AH1*Pr_H1 + Pr_AH2*Pr_H2 + Pr_AH3*Pr_H3\n\n# Conditionals - aposteriories\nPr_H1A = Pr_AH1*Pr_H1/Pr_A\nPr_H2A = Pr_AH2*Pr_H2/Pr_A\nPr_H3A = Pr_AH3*Pr_H3/Pr_A\n\n# Report\nprint ('Probability of A:', Pr_A)\nprint ('Probability od H1 at A:', Pr_H1A)\nprint ('Probability od H2 at A:', Pr_H2A)\nprint ('Probability od H3 at A:', Pr_H3A)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.2. Probability space, random variables </div>\n</div>\n\n\n## ‚ñ† Moments ‚Äì expectation and variance\n\n\n- Mathematical expectation\n    - continious distribution\n    $$ E(X) = \\int_{-\\infty}^\\infty p_X(x) dx $$\n    - discrete distribution\n    $$ E(X) = \\sum_{k} k p_k $$ \n\n\n- Moments: $k$-th moment about a value $a$\n  $$ a_k = E((X-a)^k) $$\n      \n      \n- Mathematical expectation: first moment about a $0$\n\n\n- Variance and standard deviation: second moment about expected value: \n$$ D(X) = \\sigma^2(X) = E((X - E(X))^2) $$ \n\n\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 14 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.2. Probability space, random variables </div>\n</div>\n\n\n## ‚ñ† Sequence of random variables \n\n\n- Sequence of random variables \n$$ X_1, X_2, X_n, \\ldots, X_n, \\ldots  $$ \n\n\n- Stochastic process: index is time\n\n\n- The role in TC: \n    - Traffic modeling, analysis \n    - Queuing theory\n    - User modelling\n    \n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 15 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.2. Probability space, random variables </div>\n</div>\n\n\n## ‚ñ† Selected probability distributions\n\n\n\n- Bernoulli: sequence of discrete events\n$$ p_k = {n\\choose k} p^k (1-p)^{n-k} $$\n\n\n- Normal: a sum of independent contributions\n$$ p(x; a, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x-a)^2}{2\\sigma^2}} $$\n\n\n- Chi square $\\chi^2$: a sum of squared standardized normally distributed variables\n$$ p(x; k) = \\left\\{\n  \\matrix{\\frac{1}{2^\\frac{k}{2}\\Gamma(\\frac{k}{2})} x^{\\frac{k}{2}-1} e^{-\\frac{x}{2}}, & x\\geq 0 \\cr\n   0, \\hfill & x < 0}\\right. $$\n   \n\n- Poisson: The number of independent events per time interval\n$$ p(k; \\lambda) = \\frac{\\lambda^k}{k!} e^{-\\lambda} $$\n\n\n- Exponent distribution: times among independent events distributed a Poisson\n$$ p(t; \\lambda) = \\left\\{\\matrix{\\lambda e^{-\\lambda t}, & t \\geq 0 \\cr\n 0, \\hfill & t < 0.}\\right. $$\n \n \n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 16 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.2. Probability space, random variables </div>\n</div>\n\n\n## ‚ñ† Central limit theorem\n\n\n- Sequence of random variables $X_1, X_2, \\ldots$, equal finite variances $D(X_n) = d$, and partial summs \n$$ S_n = X_1 + \\cdots + X_n, $$\nthan we have\n$$ \\frac{S_n - E(S_n)}{\\sigma(S_n)} \\quad \\underset{n\\to\\infty}{\\longrightarrow} \\quad N(0,1), $$\nkjer je $N(0,1)$ standardna normalna porazdelitev \n\n\n- This is the origin of normal distributions \n\n\n- Nature hides distributions \n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 17 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.3. Hypotheses testing </div>\n</div>\n\n\n\n## 8.3. Hypotheses testing\n\n- problem: is the observed difference coincident or reality \n- Null hypothesis $H_0$, p-value\n- Risk level $\\alpha$, conclusion on $H_0$\n- sample size determination\n\n\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 1 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.3. Hypotheses testing </div>\n</div>\n\n\n## ‚ñ† Problem and solution\n\n- **Problem:**\n    - Experimental results with previous and improved algorithms are 0.61 and 0.63 respectively\n    - Is the difference real or coincidence?\n\n\n- **Solution:**\n    - statistical hypotheses testing\n\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 1 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.3. Hypotheses testing </div>\n</div>\n\n\n## ‚ñ† Null hypothesis, p-value\n\n\n- The zero hypothesis H0 is the 'no effect'\n\n\n- p-value is the probability that the experimental result obtained is far or far away from the zero hypothesis\n\np = P [x is this much or more away from the valid $H_0|_{H0}$]\n\np-value is the probability that the zero hypothesis holds with the obtained experimental results\n\n\n- A decision on $H_0$ is based on this probability\n\n\n- How to calculate p value?: there are statistical tests that are packages\n    - zero hypothesis\n    - the equation for the degree of risk is elaborated\n    - defaults / conditions for using the test\n\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 1 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.3. Hypotheses testing </div>\n</div>\n\n\n## ‚ñ† Risk level $\\alpha$ and decision on $H_0$\n\n\n- Basic approach to decision: if the probability of zero hypothesis (p-value) is too small, we reject it\n- Select the level of risk $\\alpha$ and\n$$ p \\geq \\alpha \\qquad \\Rightarrow \\qquad H_0 \\; \\mbox{confirm} $$\n$$ p < \\alpha \\qquad \\Rightarrow \\qquad H_0 \\; \\mbox{discard} $$\n\n\n- There may be an error in the decision, this can not be avoided. We can not set the risk levels to $0$\n\n\n- The conclusion of the conclusion is analyzed as follows\n<table style=\"width:30%\" align=\"center\">\n  <tr align=\"center\">\n    <th></th>\n    <th>$\\hat{H_0}$</th> \n    <th>$\\neg\\hat{H_0}$</th>\n  </tr>\n  <tr align=\"center\">\n    <td>$H_0$</td>\n    <td>OK</td> \n    <td>Err. Type I. </td>\n  </tr>\n  <tr align=\"center\">\n    <td>$\\neg H_0$</td>\n    <td>Err. Type II.</td> \n    <td>OK</td>\n  </tr>\n</table>\n\n- Type I error:\n¬†¬†¬†¬† - we reject the null hypothesis when it holds\n¬†¬†¬†¬† - the probability of this error is surprisingly independent of the sample size $n$ and is equal to the degree of risk:\n¬†¬†¬†¬† $$ P(Err. Type I.) = \\alpha $$\n¬†¬†¬†¬†\n¬†¬†¬†¬†\n- Type II error:\n¬†¬†¬†¬† - we accept a null hypothesis as it does not hold\n¬†¬†¬†¬† - the probability of this error depends on the size of the sample, mark it\n¬†¬†¬†¬† $$ P(Err. Type II.) = \\beta $$\n\n\n- Statistical power:\n¬†¬†¬†¬† - The test power is \n¬†¬†¬†¬† $$ pw = 1 - \\ beta $$\n¬†¬†¬†¬† - this is the sensitivity of the test\n\n\n    \n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 1 </div>    \n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.3. Hypotheses testing </div>\n</div>\n\n\n## ‚ñ† Determining sample size\n\n\n- The required sample size of the test characteristics is determined on the basis of the fact that the strength of the $pw$ test depends on the sample size $n$.\n\n\n- We also need the **effect size**:\n    - this is a standardized measure for the size of the deviation from the null hypothesis, that is, the size of the difference between the tested options\n    - it is determined for each type of statistical test \n\n\n- The power of the test $pw \\in [0,1]$ increases with the sample size. The required sample size is thus determined for\n    - given effect size\n    - required test power\n\n\n- The size of the sample and the analysis of the power achieved can be determined, inter alia, with the Ordo GPower\n    - link to the tool http://www.gpower.hhu.de/en.html\n    - the example of the dependence of the achieved statistical power and the sample size determined by the Ordemo GPower is given in the following figure\n\n\n\n    \n\n<img style=\"float: center; width: 500px; margin: 0px 20px 20px 0px;\" src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/StatPowerAnalysisEx.png\">\n\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 1 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## An example of t-test\n\nimport numpy as np\nfrom scipy import stats\n\n\n## Define 2 random distributions\n#Sample Size\nN = 30\n# Standard deviations\ns1 = 1\ns2 = 1\ns = 1\n# Random samples\nx1 = s1*np.random.randn(N)\nx21 = s2*np.random.randn(N)\nx22 = s2*np.random.randn(N) + 0.1*s\nx23 = s2*np.random.randn(N) + 0.2*s\nx24 = s2*np.random.randn(N) + 0.3*s\nx25 = s2*np.random.randn(N) + 0.5*s\nx26 = s2*np.random.randn(N) + 0.8*s\nx27 = s2*np.random.randn(N) + 1.0*s\nx28 = s2*np.random.randn(N) + 2.0*s\n\n\n\n## Do the testing\nt1, p1 = stats.ttest_ind(x1, x21)\nprint(\"P value is: \" + str(p1))\nt2, p2 = stats.ttest_ind(x1, x22)\nprint(\"P value is: \" + str(p2))\nt3, p3 = stats.ttest_ind(x1, x23)\nprint(\"P value is: \" + str(p3))\nt4, p4 = stats.ttest_ind(x1, x24)\nprint(\"P value is: \" + str(p4))\nt5, p5 = stats.ttest_ind(x1, x25)\nprint(\"P value is: \" + str(p5))\nt6, p6 = stats.ttest_ind(x1, x26)\nprint(\"P value is: \" + str(p6))\nt7, p7 = stats.ttest_ind(x1, x27)\nprint(\"P value is: \" + str(p7))\nt8, p8 = stats.ttest_ind(x1, x28)\nprint(\"P value is: \" + str(p8))",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "P value is: 0.4008730199234859\nP value is: 0.6439588505971517\nP value is: 0.8046418690393277\nP value is: 0.3252356269398261\nP value is: 0.614755211144906\nP value is: 0.0003852585762219998\nP value is: 0.004210777374247186\nP value is: 6.467026996083114e-13\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.3. Hypotheses testing </div>\n</div>\n\n\n## ‚ñ† Selection and implementation of statistical test\n\n- any statistical significant test requires conditions on analysed data to be met, otherwise its result is not valid. For example. The t-test requires at least an interval variable, both of which are normally divisible.\n\n\n\n- the conditions of the statistical test:\n¬†¬†¬†¬† - sample size\n¬†¬†¬†¬† - types of input data (variables): nominal (categorical), ordinal, interval, proportional\n¬†¬†¬†¬† - the distribution of input data variables: normal or not?\n¬†¬†¬†¬† - additional conditions\n     \n     \n- an example of statistical test hierarchy: comparing means     \n\n<table style=\"width:40%\" align=\"center\">\n  <tr>\n    <th></th>\n    <th>Compare means</th> \n    <th></th>\n  </tr>\n  <tr>\n    <td style='text-align:center'>$\\swarrow$</td>\n    <td></td> \n    <td style='text-align:center'>$\\searrow$</td>\n  </tr>\n  <tr>\n    <td style='text-align:center'>t-test (indep.)</td>\n    <td></td> \n    <td style='text-align:center'>t-test (dep.)</td>\n  </tr>  \n  <tr>\n    <td style='text-align:center'>$\\downarrow$</td>\n    <td></td> \n    <td style='text-align:center'>$\\downarrow$</td>\n  </tr>  \n  <tr>\n    <td style='text-align:center'>Man-Whitney U</td>\n    <td></td> \n    <td style='text-align:center'>Sign test</td>\n  </tr> \n  <tr>\n    <td style='text-align:center'>$\\downarrow$</td>\n    <td></td> \n    <td style='text-align:center'>$\\downarrow$</td>\n  </tr>\n  <tr>\n    <td style='text-align:center'>$\\chi^2$-test</td>\n    <td></td> \n    <td style='text-align:center'>$\\chi^2$-test</td>\n  </tr>   \n</table>\n\n\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 1 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n<div style=\"flex:1;width:50%;\"> 8. Elements of probability theory  </div>\n<div style=\"flex:1;width:50%;text-align:right;\"> 8.3. Hypotheses testing </div>\n</div>\n\n\n## ‚ñ† Conclusion\n\n- Events and probability: a model of phenomena for which there is no known deterministic outcome\n- Important for giving the characteristics of TK systems \"in the long run\"\n\n\n\n- Application:\n     - Markov chain\n     - Time series\n     - Experiments with end users\n\n\n\n<p style=\"margin-bottom:2cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 1 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}